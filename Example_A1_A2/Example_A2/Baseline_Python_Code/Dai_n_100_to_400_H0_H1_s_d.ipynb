{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HKf_0qRTcNOO","executionInfo":{"status":"ok","timestamp":1721257142623,"user_tz":300,"elapsed":21055,"user":{"displayName":"Linjun","userId":"13995434670626730968"}},"outputId":"cd8787b4-c677-49df-d261-0351c73c4c74"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2hSHv7pHVT1"},"outputs":[],"source":["import torch\n","import matplotlib.pyplot as plt\n","import torch.utils.data as Data\n","import torch.nn as nn\n","import numpy as np\n","from scipy.stats import norm\n","from scipy.stats import chi2\n","import random\n","from scipy.linalg import sqrtm\n","from scipy.linalg import toeplitz\n","from scipy.spatial.distance import pdist, squareform\n","import math\n","from sklearn.model_selection import KFold\n","from torchvision import models\n","\n","\n","\n","#network\n","class Net(nn.Module):\n","    def __init__(self,n_features,num=25, n_out=1,rate = 0):\n","        super(Net,self).__init__()\n","        self.out = nn.Sequential(\n","\n","            nn.Dropout(rate),\n","            nn.Linear(n_features,num),\n","            nn.ReLU(),\n","\n","            nn.Dropout(rate),\n","            nn.Linear(num, num),\n","            nn.ReLU(),\n","\n","            nn.Dropout(rate),\n","            nn.Linear(num, num),\n","            nn.ReLU(),\n","\n","            nn.Dropout(rate),\n","            nn.Linear(num, num),\n","            nn.ReLU(),\n","\n","            nn.Linear(num, n_out)\n","        )\n","    def forward(self,x):\n","        x = self.out(x)\n","        return x\n","\n","def setup_seed(seed):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","\n","#early stopping\n","class EarlyStopping:\n","    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n","    def __init__(self, patience=20, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n","        \"\"\"\n","        Args:\n","            patience (int): How long to wait after last time validation loss improved.\n","                            Default: 20\n","            verbose (bool): If True, prints a message for each validation loss improvement.\n","                            Default: False\n","            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n","                            Default: 0\n","            path (str): Path for the checkpoint to be saved to.\n","                            Default: 'checkpoint.pt'\n","            trace_func (function): trace print function.\n","                            Default: print\n","        \"\"\"\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","        self.path = path\n","        self.trace_func = trace_func\n","    def __call__(self, val_loss, model):\n","\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            #self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model):\n","        '''Saves model when validation loss decrease.'''\n","        if self.verbose:\n","            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n","        torch.save(model.state_dict(), self.path)\n","        self.val_loss_min = val_loss\n","\n","#Example A1\n","def Creat_data(n,p1,p2, case, a, rho=0.3):\n","    p = p1+p2\n","    p0 = p//2\n","    temp = rho**np.linspace(0,p-1,p)\n","    Sigma =  toeplitz(temp,temp)\n","    Sigmasqrt =  torch.from_numpy(sqrtm(Sigma)).type(torch.FloatTensor)\n","    x = torch.randn(n,p)\n","    x = torch.mm(x,Sigmasqrt)\n","    if case == 1:#sparse\n","        beta = torch.cat([torch.ones(2,1),torch.zeros(p1-2,1),torch.ones(2,1)*a/math.sqrt(2),torch.zeros(p2-2,1)],dim=0)\n","        epsilon = torch.randn(n,1)*0.5 #homo\n","        #epsilon = torch.randn(n,1)*torch.unsqueeze(torch.exp(x[:,2])/(1+torch.exp(x[:,2])),dim=1) #hete\n","        y = torch.mm(x,beta)+epsilon\n","\n","    elif case == 2:#dense\n","        beta = torch.cat([torch.ones(2,1),torch.zeros(p1-2,1),torch.ones(p2,1)*a/math.sqrt(p2)],dim=0)\n","        epsilon = torch.randn(n,1)*0.5\n","        #epsilon = torch.randn(n,1)*torch.unsqueeze(torch.exp(x[:,2])/(1+torch.exp(x[:,2])),dim=1) #hete\n","        y = torch.mm(x,beta)+epsilon\n","\n","    x = (x - torch.mean(x,dim=0))/torch.std(x,dim=0)\n","    y = (y-torch.mean(y))/torch.std(y)\n","    z = x[:,:p1]\n","    w = x[:,p1:]\n","    return z,w,y\n","\n","#Example A2\n","def Creat_data2(n,p1,p2, case, a, rho=0.3):\n","    p = p1+p2\n","    p0 = p//2\n","    temp = rho**np.linspace(0,p-1,p)\n","    Sigma =  toeplitz(temp,temp)\n","    Sigmasqrt =  torch.from_numpy(sqrtm(Sigma)).type(torch.FloatTensor)\n","    x = torch.randn(n,p)\n","    x = torch.mm(x,Sigmasqrt)\n","    z = x[:,:p1]\n","    w = x[:,p1:]\n","    if case == 1:#sparse\n","        beta = torch.cat([torch.ones(5,1)*a/math.sqrt(5),torch.zeros(p2-5,1)],dim=0)#####\n","        epsilon = torch.randn(n,1)*0.5\n","        y = torch.unsqueeze(z[:,0]+z[:,1],dim=1) +torch.mm(w,beta)**2+epsilon\n","\n","    elif case == 2:#dense\n","        beta = torch.cat([torch.ones(int(np.floor(p2/2)),1)*a/math.sqrt(int(np.floor(p2/2))), torch.zeros(p2-int(np.floor(p2/2)),1)],dim=0)\n","        epsilon = torch.randn(n,1)*0.5\n","        y = torch.unsqueeze(z[:,0]+z[:,1],dim=1) +torch.mm(w,beta)**2+epsilon\n","\n","    z = x[:,:p1]\n","    w = x[:,p1:]\n","    return z,w,y\n","\n","#Example A3\n","def Creat_data3(n,p1,p2, case, a, rho=0.3):\n","    p = p1+p2\n","    p0 = p//2\n","    temp = rho**np.linspace(0,p-1,p)\n","    Sigma =  toeplitz(temp,temp)\n","    Sigmasqrt =  torch.from_numpy(sqrtm(Sigma)).type(torch.FloatTensor)\n","    x = torch.randn(n,p)\n","    x = torch.mm(x,Sigmasqrt)\n","    z = x[:,:p1]\n","    w = x[:,p1:]\n","    if case == 1:#sparse\n","        beta = torch.cat([torch.ones(2,1)*a,torch.zeros(p2-2,1)],dim=0)\n","        epsilon = torch.randn(n,1)*0.5\n","        y = torch.unsqueeze(z[:,0]-z[:,1],dim=1) + torch.mm(torch.exp(w),beta)+epsilon\n","\n","    elif case == 2:#dense\n","        beta = torch.cat([torch.ones(p2,1)*a],dim=0)\n","        epsilon = torch.randn(n,1)*0.5\n","        y = torch.unsqueeze(z[:,0]-z[:,1],dim=1) + torch.mm(torch.exp(w),beta)+epsilon\n","    z = x[:,:p1]\n","    w = x[:,p1:]\n","    return z,w,y\n","\n","#single split\n","def test1(z,w,y,c):\n","    n = z.shape[0]\n","    p1 = z.shape[1]\n","    p2 = w.shape[1]\n","    x = torch.cat([z,w],1)\n","    p = p1+p2\n","    n1 = math.ceil(c*n)\n","\n","    EPOCH = 200\n","    BATCH_SIZE =50\n","    LR = 0.003#0.001\n","    num = 50\n","\n","    index = np.random.choice(a=n, size=n1, replace=False)\n","    x_train = x[index,:]\n","    x_test = np.delete(x,index,axis=0)\n","    z_train = z[index,:]\n","    z_test = np.delete(z,index,axis=0)\n","    y_train = y[index,:]\n","    y_test = np.delete(y,index,axis=0)\n","\n","    net1= Net(p1,num=num,rate = 0)\n","    loss_func=nn.MSELoss()\n","    optimizer=torch.optim.Adam(net1.parameters(),lr=LR,weight_decay = 1e-8)\n","    early_stopping = EarlyStopping(patience=EPOCH, verbose=False)\n","    train_result = np.zeros(EPOCH)\n","    test_result = np.zeros(EPOCH)\n","    train_data = Data.TensorDataset(z_train , y_train)\n","    train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n","    #----------------------------------------------------------------\n","    for epoch in range(EPOCH):\n","        #---------------------------------------------------\n","        net1.train()\n","        for step, (b_x, b_y) in enumerate(train_loader):\n","            predicton=net1(b_x)\n","            loss=loss_func(predicton,b_y)\n","            optimizer.zero_grad()           # clear gradients for this training step\n","            loss.backward()                 # backpropagation, compute gradients\n","            optimizer.step()                # apply gradients\n","\n","        net1.eval()\n","        train_result[epoch] = loss_func(net1(z_train),y_train )\n","        valid_loss = loss_func(net1(z_test),y_test )\n","        test_result[epoch] = valid_loss\n","        #if (epoch+1)%10==0:\n","            #print('Epoch: ', epoch+1, '| train loss: %.4f' % loss.data.numpy() )\n","        early_stopping(valid_loss, net1)\n","\n","        if early_stopping.early_stop:\n","            break\n","\n","    net1.load_state_dict(torch.load('checkpoint.pt'))\n","\n","    net1.eval()\n","    h_train = net1(z_train)\n","    y_tilde_train = y_train - h_train\n","    h_test = net1(z_test)\n","    y_tilde_test = y_test - h_test\n","\n","    net2 = Net(p,num=num,rate=0)\n","    BATCH_SIZE = n1\n","    loss_func=nn.MSELoss()\n","    optimizer=torch.optim.Adam(net2.parameters(),lr=LR,weight_decay = 1e-4)\n","    early_stopping = EarlyStopping(patience=7, verbose=False)\n","    train_result = np.zeros(EPOCH)\n","    test_result = np.zeros(EPOCH)\n","    train_data2 = Data.TensorDataset(x_train, y_tilde_train)\n","    train_loader2 = Data.DataLoader(dataset=train_data2, batch_size=BATCH_SIZE, shuffle=True)\n","    #----------------------------------------------------------------\n","    for epoch in range(EPOCH):\n","        #---------------------------------------------------\n","        net2.train()\n","        for step, (b_x, b_y) in enumerate(train_loader2):\n","            predicton=net2(b_x)\n","            loss=loss_func(predicton,b_y)\n","            optimizer.zero_grad()           # clear gradients for this training step\n","            loss.backward(retain_graph=True)                 # backpropagation, compute gradients\n","            optimizer.step()                # apply gradients\n","\n","        net2.eval()\n","        train_result[epoch] = loss_func(net2(x_train),y_tilde_train)\n","        valid_loss = loss_func(net2(x_test),y_tilde_test)\n","        test_result[epoch] = valid_loss\n","        early_stopping(valid_loss, net2)\n","\n","        if early_stopping.early_stop:\n","            #print(\"Early stopping\")\n","            break\n","\n","    net2.load_state_dict(torch.load('checkpoint.pt'))\n","\n","    net2.eval()\n","    f_hat = net2(x_test).detach().numpy()\n","    Tn = np.sum((f_hat-np.mean(y_tilde_test.detach().numpy()))**2)/np.var( y_tilde_test.detach().numpy() )\n","    Tn_enhance = Tn+np.sum(f_hat**2)\n","    return([Tn,Tn_enhance])\n","\n","####select the splitting ratio\n","def choose_c(z,w,y):\n","    c_list = [3/4,4/5,5/6,6/7,7/8,8/9,9/10,19/20]\n","    c_num = len(c_list)\n","    n = z.shape[0]\n","    p1 = z.shape[1]\n","    p2 = w.shape[1]\n","    x = torch.cat([z,w],1)\n","    p = p1+p2\n","\n","    MM = 200\n","    temp = np.zeros(MM)\n","    ans = np.zeros(c_num)\n","    j = 0\n","    for c in range(c_num):\n","        cc = c_list[c]\n","        for m in range(MM):\n","            permu = np.random.choice(a=n, size=n, replace=False)\n","            ww = w[permu,:]\n","            temp[m] = test1(z,ww,y,cc)[0]\n","            #print(\"\\r\",cc,'  ---  times:', m+1,end=\"\",flush=True)\n","        #print('\\n',cc,' --- ',np.mean(temp>chi2.ppf(0.95,df=1)))\n","        ans[j] = np.mean(temp>chi2.ppf(0.95,df=1))\n","        if ans[j]<=0.05:\n","            return (cc)\n","        j = j+1\n","\n","    if min(ans)>0.05:\n","        return (c_list[np.argmin(ans)])\n","\n","#Multiple data splitting test\n","def Cauchy_test1(z,w,y,c,B=5):\n","    pv = np.zeros([2,B])\n","    Tx = np.zeros([2,B])\n","    for b in range(B):\n","        test_result = test1(z,w,y,c)\n","        pv[0,b] = 1- chi2.cdf(test_result[0],df=1)\n","        pv[1,b] = 1- chi2.cdf(test_result[1],df=1)\n","        Tx[0,b] = np.tan((1/2-pv[0,b])*np.pi)\n","        Tx[1,b] = np.tan((1/2-pv[1,b])*np.pi)\n","    Tmean=np.mean(Tx,1)\n","    Q = 1/2- np.arctan(Tmean)/np.pi\n","    return(Q)\n","\n","\n","\n","#single split\n","def Dai(z,w,y,c=0.5,r = 0.01):\n","    n = z.shape[0]\n","    p1 = z.shape[1]\n","    p2 = w.shape[1]\n","    x = torch.cat([z,w],1)\n","    p = p1+p2\n","    n1 = math.ceil(c*n)\n","\n","    EPOCH = 200\n","    BATCH_SIZE =50\n","    LR = 0.003\n","    num = 50\n","\n","    index = np.random.choice(a=n, size=n1, replace=False)\n","    x_train = x[index,:]\n","    x_test = np.delete(x,index,axis=0)\n","\n","    z = torch.cat([z,torch.zeros(n,p2)],1)\n","    z_train = z[index,:]\n","    z_test = np.delete(z,index,axis=0)\n","\n","    y_train = y[index,:]\n","    y_test = np.delete(y,index,axis=0)\n","\n","    net1= Net(p,num=num,rate = 0)\n","    loss_func=nn.MSELoss()\n","    optimizer=torch.optim.Adam(net1.parameters(),lr=LR,weight_decay = 1e-4)\n","    early_stopping = EarlyStopping(patience=20, verbose=False)\n","    train_result = np.zeros(EPOCH)\n","    test_result = np.zeros(EPOCH)\n","    train_data = Data.TensorDataset(z_train , y_train)\n","    train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n","    #----------------------------------------------------------------\n","    for epoch in range(EPOCH):\n","        #---------------------------------------------------\n","        net1.train()\n","        for step, (b_x, b_y) in enumerate(train_loader):\n","            predicton=net1(b_x)\n","            loss=loss_func(predicton,b_y)\n","            optimizer.zero_grad()           # clear gradients for this training step\n","            loss.backward()                 # backpropagation, compute gradients\n","            optimizer.step()                # apply gradients\n","\n","        net1.eval()\n","        train_result[epoch] = loss_func(net1(z_train),y_train )\n","        valid_loss = loss_func(net1(z_test),y_test )\n","        test_result[epoch] = valid_loss\n","        early_stopping(valid_loss, net1)\n","        if early_stopping.early_stop:\n","            break\n","\n","    net1.load_state_dict(torch.load('checkpoint.pt'))\n","\n","    net1.eval()\n","    h_test = net1(z_test)\n","\n","    net2 = Net(p,num=num,rate=0)\n","    loss_func=nn.MSELoss()\n","    optimizer=torch.optim.Adam(net2.parameters(),lr=LR,weight_decay = 1e-4)\n","    early_stopping = EarlyStopping(patience=20, verbose=False)\n","    train_result = np.zeros(EPOCH)\n","    test_result = np.zeros(EPOCH)\n","    train_data2 = Data.TensorDataset(x_train, y_train)\n","    train_loader2 = Data.DataLoader(dataset=train_data2, batch_size=BATCH_SIZE, shuffle=True)\n","    #----------------------------------------------------------------\n","    for epoch in range(EPOCH):\n","        #---------------------------------------------------\n","        net2.train()\n","\n","        for step, (b_x, b_y) in enumerate(train_loader2):\n","            predicton=net2(b_x)\n","            loss=loss_func(predicton,b_y)\n","            optimizer.zero_grad()           # clear gradients for this training step\n","            loss.backward(retain_graph=True)                 # backpropagation, compute gradients\n","            optimizer.step()                # apply gradients\n","\n","        net2.eval()\n","        train_result[epoch] = loss_func(net2(x_train),y_train)\n","        valid_loss = loss_func(net2(x_test),y_test)\n","        test_result[epoch] = valid_loss\n","        #if (epoch+1)%10==0:\n","            #print('Epoch: ', epoch+1, '| train loss: %.4f' % loss.data.numpy() )\n","        early_stopping(valid_loss, net2)\n","        if early_stopping.early_stop:\n","            #print(\"Early stopping\")\n","            break\n","\n","    net2.load_state_dict(torch.load('checkpoint.pt'))\n","\n","    net2.eval()\n","    f_hat = net2(x_test).detach().numpy()\n","\n","    n2 = n - n1\n","    delta = ((y_test-f_hat)**2 - (y_test-h_test)**2 + torch.randn(n2,1)*r).detach().numpy()\n","    return(np.sum(delta)/n2**(0.5)/np.std(delta))\n","\n","\n","def Cauchy_Dai(z,w,y,c,r,B=5):\n","    pv = np.zeros(B)\n","    Tx = np.zeros(B)\n","    for b in range(B):\n","        pv[b] = norm.cdf(Dai(z,w,y,c,r))\n","        Tx[b] = np.tan((1/2-pv[b])*np.pi)\n","    Tmean=np.mean(Tx)\n","    Q = 1/2- np.arctan(Tmean)/np.pi\n","    return(Q)\n","\n","setup_seed(123)\n","MCMC = 500\n","\n","n_list = [400, 300, 200, 100]\n","\n","# n = 100\n","for n in n_list:\n","    result = np.zeros([MCMC,3])\n","    for xxx in range(MCMC):\n","        z,w,y = Creat_data2(n,25,25,case=1,a=0,rho=0.3)\n","        result[xxx,0] = Dai(z,w,y,c=0.5,r=0.01)\n","        z,w,y = Creat_data2(n,25,25,case=1,a=0.5,rho=0.3)\n","        result[xxx,1] = Dai(z,w,y,c=0.5,r=0.01)\n","        z,w,y = Creat_data2(n,25,25,case=2,a=0.5,rho=0.3)\n","        result[xxx,2] = Dai(z,w,y,c=0.5,r=0.01)\n","        print(\"\\r\",'  times:', xxx+1,'  ---  ',np.sum(result[0:(xxx+1),:]<norm.ppf(0.05),axis=0)/(xxx+1),end=\"\",flush=True)\n","    print('\\n',np.sum(result<norm.ppf(0.05),axis=0)/MCMC)\n","\n","\n","    result2 = np.zeros([MCMC,3])\n","    for xxx in range(MCMC):\n","        z,w,y = Creat_data2(n,25,25,case=1,a=0,rho=0.3)\n","        result2[xxx,0] = Cauchy_Dai(z,w,y,c=0.5,r=0.01)\n","        z,w,y = Creat_data2(n,25,25,case=1,a=0.5,rho=0.3)\n","        result2[xxx,1] = Cauchy_Dai(z,w,y,c=0.5,r=0.01)\n","        z,w,y = Creat_data2(n,25,25,case=2,a=0.5,rho=0.3)\n","        result2[xxx,2] = Cauchy_Dai(z,w,y,c=0.5,r=0.01)\n","        print(\"\\r\",'  times:', xxx+1,'  ---  ',np.sum(result2[0:(xxx+1),:]<0.05,axis=0)/(xxx+1),end=\"\",flush=True)\n","    print('\\n',np.sum(result2<0.05,axis=0)/MCMC)\n","\n","    np.savetxt(\"/content/drive/My Drive/Mean_Indep_baseline/Example_A2_Dai_H0_H1_s1_d1_ans_n_\" + str(n) + \"_python.csv\", result, delimiter=\",\")\n","    np.savetxt(\"/content/drive/My Drive/Mean_Indep_baseline/Example_A2_Cauchy_Dai_H0_H1_s1_d1_ans_n_\" + str(n) + \"_python.csv\", result2, delimiter=\",\")"]}]}