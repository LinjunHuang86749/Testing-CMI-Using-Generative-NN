{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"cycUBKuQkltO"},"outputs":[],"source":["input_n_test = 500\n","input_sample_size = 400 # 200, 400, 600, 800\n","input_seed = 42\n","\n","import math\n","import torch\n","import torch.distributions as TD\n","from torch.utils.data import Dataset, DataLoader\n","from zmq import device\n","import torch.optim as optim\n","import numpy as np\n","from datetime import datetime\n","import functools\n","from scipy.linalg import toeplitz\n","import xgboost as xgb\n","from sklearn.linear_model import LassoCV\n","from tqdm import tqdm\n","\n","# Move model on GPU if available\n","enable_cuda = True\n","device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')\n","\n","def get_xzy_randn_nl(n_points, ground_truth='H0', rho=0.3, p1 = 25, p2 = 25, device='cuda:0', a = 0.0, **ignored):\n","    p = p1 + p2\n","    cov_vec = rho ** np.arange(p)\n","    cov_mx = torch.FloatTensor(toeplitz(cov_vec)).to(device)\n","    X_all_generator = TD.MultivariateNormal(torch.zeros(p).to(device), cov_mx)\n","    X_all = X_all_generator.sample((n_points,))\n","    Z = X_all[:, :p1]\n","    X = X_all[:, p1:]\n","\n","    beta_z = torch.FloatTensor([1] * 2 + [0] * (p1 - 2)).reshape((-1, 1)).to(device)\n","\n","    if ground_truth == 'H0':\n","\n","        beta_x = torch.FloatTensor([0] * 2 + [0] * (p2 - 2)).reshape((-1, 1)).to(device)\n","\n","    elif ground_truth == 'H1_sparse':\n","\n","        beta_x = torch.FloatTensor([a/math.sqrt(5)] * 5 + [0] * (p2 - 5)).reshape((-1, 1)).to(device)\n","\n","    elif ground_truth == 'H1_dense':\n","\n","        beta_x = torch.FloatTensor([a/math.sqrt(12)] * 12 + [0] * (p2 - 12)).reshape((-1, 1)).to(device)\n","\n","    else:\n","        raise NotImplementedError(f'{ground_truth} has to be H0, H1_sparse or H1_dense')\n","\n","    epsilon = torch.randn(n_points, 1, device=device)*0.5\n","    Y = torch.matmul(Z, beta_z) + torch.matmul(X, beta_x) ** 2 + epsilon\n","\n","    return X, Y, Z\n","\n","def get_xzy_randn_nl_fix(Z, n_points, ground_truth='H0', rho=0.3, p1 = 25, p2 = 25, device='cuda:0', a = 0.0, **ignored):\n","    p = p1 + p2\n","    cov_vec = rho ** np.arange(p)\n","    cov_mx = torch.FloatTensor(toeplitz(cov_vec)).to(device)\n","\n","    Cov_11 = cov_mx[:p1,:p1]\n","    Cov_12 = cov_mx[:p1,p1:]\n","    Cov_21 = cov_mx[p1:,:p1]\n","    Cov_22 = cov_mx[p1:,p1:]\n","\n","\n","    if ground_truth == 'H0':\n","\n","        beta_x = torch.FloatTensor([0] * 2 + [0] * (p2 - 2)).reshape((-1, 1)).to(device)\n","\n","    elif ground_truth == 'H1_sparse':\n","\n","        beta_x = torch.FloatTensor([a/math.sqrt(5)] * 5 + [0] * (p2 - 5)).reshape((-1, 1)).to(device)\n","\n","    elif ground_truth == 'H1_dense':\n","\n","        beta_x = torch.FloatTensor([a/math.sqrt(12)] * 12 + [0] * (p2 - 12)).reshape((-1, 1)).to(device)\n","\n","    else:\n","        raise NotImplementedError(f'{ground_truth} has to be H0, H1_sparse or H1_dense')\n","\n","    beta_z = torch.FloatTensor([1] * 2 + [0] * (p1 - 2)).reshape((-1, 1)).to(device)\n","\n","    Cov_22_inv = torch.inverse(Cov_22)\n","    Condi_Cov = Cov_11 - torch.matmul(torch.matmul(Cov_12, Cov_22_inv), Cov_21)\n","    Condi_Mean_vec = torch.matmul(torch.matmul(Cov_12, Cov_22_inv), Z.T)\n","    temp_generator = TD.MultivariateNormal(Condi_Mean_vec, Condi_Cov)\n","    X = temp_generator.sample((n_points,))\n","\n","    first_term = torch.matmul(beta_z.T, Z.reshape((-1, 1)))\n","\n","    var_term = Cov_11 -  torch.matmul(torch.matmul(Cov_12, Cov_22_inv), Cov_21)\n","    sec_term = torch.matmul(torch.matmul(beta_x.T, var_term), beta_x)\n","\n","    condi_exp_term = torch.matmul(torch.matmul(Cov_12, Cov_22_inv), Z.reshape((-1, 1)))\n","    third_term_temp = torch.matmul(condi_exp_term, condi_exp_term.T)\n","    third_term = torch.matmul(torch.matmul(beta_x.T, third_term_temp), beta_x)\n","\n","    Y = first_term + sec_term + third_term\n","\n","    return X, Y.reshape(-1)\n","\n","\n","def get_p_value_stat(boot_num, M, n, gen_x_all_torch, gen_y_all_torch, x_torch, y_torch, z_torch, boor_rv_type=\"gaussian\"):\n","\n","    d_y = y_torch.shape[1]\n","    d_x = x_torch.shape[1]\n","\n","    w_mx = torch.linalg.vector_norm(z_torch.repeat(n,1,1) - torch.swapaxes(z_torch.repeat(n,1,1), 0, 1), ord = 1, dim = 2)\n","    sigma_w = torch.median(w_mx).item()\n","\n","    u_mx = torch.linalg.vector_norm(x_torch.repeat(n,1,1) - torch.swapaxes(x_torch.repeat(n,1,1), 0, 1), ord = 1, dim = 2)\n","    sigma_u = torch.median(u_mx).item()\n","\n","    # print(sigma_w)\n","    # print(sigma_u)\n","\n","    # sigma_w, sigma_u = 1.0, 1.0\n","\n","    w_mx = torch.linalg.vector_norm(z_torch.repeat(n, 1, 1) - torch.swapaxes(z_torch.repeat(n, 1, 1), 0, 1), ord=1, dim=2)\n","    w_mx = torch.exp(-w_mx / sigma_w)\n","\n","    u_mx_1 = torch.exp(-torch.linalg.vector_norm(x_torch.repeat(n, 1, 1) - torch.swapaxes(x_torch.repeat(n, 1, 1), 0, 1), ord=1, dim=2) / sigma_u)\n","    u_mx_2 = torch.mean(\n","        torch.exp(-torch.linalg.vector_norm(gen_x_all_torch.repeat(n, 1, 1).reshape(n, n, -1, d_x) - x_torch.repeat(1, n).reshape(n, n, 1, d_x), ord=1, dim=3) / sigma_u), dim=2)\n","    u_mx_3 = u_mx_2.T\n","\n","    gen_x_all_torch_rep = gen_x_all_torch.repeat(n, 1, 1).reshape(n, n, -1, d_x)\n","\n","    u_mx_4 = torch.mean(torch.exp(-torch.linalg.vector_norm(gen_x_all_torch_rep - torch.swapaxes(gen_x_all_torch_rep, 0, 1), ord=1, dim=3) / sigma_u) , dim=2)\n","\n","    u_mx = u_mx_1 - u_mx_2 - u_mx_3 + u_mx_4\n","    v_mx_temp = (gen_y_all_torch - y_torch)\n","    v_mx = torch.matmul(v_mx_temp, v_mx_temp.T)\n","    FF_mx = u_mx * v_mx * w_mx * (1 - torch.eye(n).to(device))\n","\n","    stat = 1 / (n - 1) * torch.sum(FF_mx).item()\n","\n","    boottemp = np.array([])\n","    if boor_rv_type == \"rademacher\":\n","        eboot = torch.sign(torch.randn(n, boot_num)).to(device)\n","    elif boor_rv_type == \"gaussian\":\n","        eboot = torch.randn(n, boot_num).to(device)\n","    for bb in range(boot_num):\n","        random_mx = torch.matmul(eboot[:, bb].reshape(-1, 1), eboot[:, bb].reshape(-1, 1).T)\n","        bootmatrix = FF_mx * random_mx\n","        stat_boot = 1 / (n - 1) * torch.sum(bootmatrix).item()\n","        boottemp = np.append(boottemp, stat_boot)\n","    return stat, boottemp\n","\n","\n","class DatasetSelect(Dataset):\n","    def __init__(self, X, Y, Z):\n","        self.X_real = X\n","        self.Y_real = Y\n","        self.Z_real = Z\n","        self.sample_size = X.shape[0]\n","\n","    def __len__(self):\n","        return self.sample_size\n","\n","    def __getitem__(self, index):\n","        return self.X_real[index], self.Y_real[index], self.Z_real[index]\n","\n","# Create a DataLoader for given (X, Y)\n","\n","class DatasetSelect_GAN(torch.utils.data.Dataset):\n","\n","  def __init__(self, X, Y, Z, batch_size):\n","    self.X_real = X\n","    self.Y_real = Y\n","    self.Z_real = Z\n","    self.batch_size = batch_size\n","    self.sample_size = X.shape[0]\n","\n","  def __len__(self):\n","    return self.sample_size\n","\n","  def __getitem__(self, index):\n","    return self.X_real[index], self.Y_real[index], self.Z_real[index], self.Z_real[(self.batch_size+index) % self.sample_size]\n","\n","# Create a DataLoader for given (X, Y)\n","\n","class DatasetSelect_GAN_ver2(torch.utils.data.Dataset):\n","\n","  def __init__(self, Y, Z, batch_size):\n","    self.Y_real = Y\n","    self.Z_real = Z\n","    self.batch_size = batch_size\n","    self.sample_size = Z.shape[0]\n","\n","  def __len__(self):\n","    return self.sample_size\n","\n","  def __getitem__(self, index):\n","    return self.Y_real[index], self.Z_real[index]\n","\n","##### Auxilliary functions #####\n","\n","def sample_noise(sample_size, noise_dimension, noise_type, input_var):\n","\n","    if (noise_type == \"normal\"):\n","      noise_generator = TD.MultivariateNormal(\n","        torch.zeros(noise_dimension).to(device), input_var * torch.eye(noise_dimension).to(device))\n","\n","      Z = noise_generator.sample((sample_size,))\n","    if (noise_type == \"unif\"):\n","      Z = torch.rand(sample_size, noise_dimension)\n","    if (noise_type == \"Cauchy\"):\n","      Z = TD.Cauchy(torch.tensor([0.0]), torch.tensor([1.0])).sample((sample_size, noise_dimension)).squeeze(2)\n","\n","    return Z\n","\n","##### GAN architecture #####\n","\n","class Generator(torch.nn.Module):\n","\n","    def __init__(self, input_dimension, output_dimension, noise_dimension, hidden_layer_size, BN_type, ReLU_coef, drop_out_p,\n","                 drop_input = False):\n","      super(Generator, self).__init__()\n","      self.BN_type = BN_type\n","      self.ReLU_coef = ReLU_coef\n","      self.fc1 = torch.nn.Linear(input_dimension + noise_dimension, hidden_layer_size, bias=True)\n","      if BN_type:\n","        self.BN1 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","        self.BN2 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","        self.BN3 = torch.nn.BatchNorm1d(hidden_layer_size, 0.8, affine=False)\n","      self.leakyReLU1 = torch.nn.LeakyReLU(ReLU_coef)\n","      self.fc2 = torch.nn.Linear(hidden_layer_size, hidden_layer_size, bias=True)\n","      self.fc3 = torch.nn.Linear(hidden_layer_size, hidden_layer_size, bias=True)\n","      self.fc_last = torch.nn.Linear(hidden_layer_size, output_dimension, bias=True)\n","      self.fc_temp = torch.nn.Linear(input_dimension + noise_dimension, output_dimension, bias=True)\n","      self.sigmoid = torch.nn.Sigmoid()\n","      self.drop_out0 = torch.nn.Dropout(p=drop_out_p)\n","      self.drop_out1 = torch.nn.Dropout(p=drop_out_p)\n","      self.drop_out2 = torch.nn.Dropout(p=drop_out_p)\n","      self.drop_out3 = torch.nn.Dropout(p=drop_out_p)\n","      self.drop_input = drop_input\n","\n","    def forward(self, x):\n","      if self.BN_type:\n","        x = self.drop_out1(self.leakyReLU1(self.BN1(self.fc1(x))))\n","        # x = self.drop_out2(self.leakyReLU1(self.BN2(self.fc2(x))))\n","        # x = self.drop_out3(self.leakyReLU1(self.BN3(self.fc3(x))))\n","        x = self.fc_last(x)\n","\n","      else:\n","        x = self.drop_out1(self.leakyReLU1(self.fc1(x)))\n","        # x = self.drop_out2(self.leakyReLU1(self.fc2(x)))\n","        # x = self.drop_out3(self.leakyReLU1(self.fc3(x)))\n","        x = self.fc_last(x)\n","        # x = self.sigmoid(x)\n","      return x\n","\n","##### Training procedures #####\n","\n","\n","def find_loss_l(y_torch, gen_y_all_torch, z_torch, sigma_w, sigma_u, M):\n","    n = z_torch.shape[0]\n","    d_y = y_torch.shape[1]\n","    w_mx = torch.linalg.vector_norm(z_torch.repeat(n, 1, 1) - torch.swapaxes(z_torch.repeat(n, 1, 1), 0, 1), ord=1, dim=2)\n","    w_mx = torch.exp(-w_mx / sigma_w)\n","\n","    u_mx_1 = torch.exp(-torch.linalg.vector_norm(y_torch.repeat(n, 1, 1) - torch.swapaxes(y_torch.repeat(n, 1, 1), 0, 1), ord=1, dim=2) / sigma_u)\n","    u_mx_2 = torch.mean(\n","        torch.exp(-torch.linalg.vector_norm(gen_y_all_torch.repeat(n, 1, 1).reshape(n, n, -1, d_y) - y_torch.repeat(1, n).reshape(n, n, 1, d_y), ord=1, dim=3) / sigma_u), dim=2)\n","    u_mx_3 = u_mx_2.T\n","\n","    gen_y_all_torch_rep = gen_y_all_torch.repeat(n, 1, 1).reshape(n, n, -1, d_y)\n","\n","    u_mx_4 = torch.mean(torch.exp(-torch.linalg.vector_norm(gen_y_all_torch_rep - torch.swapaxes(gen_y_all_torch_rep, 0, 1), ord=1, dim=3) / sigma_u), dim=2)\n","\n","    u_mx = u_mx_1 - u_mx_2 - u_mx_3 + u_mx_4\n","\n","    FF_mx = u_mx * w_mx * (1 - torch.eye(n).to(device))\n","\n","    loss = 1 / (n) * torch.sum(FF_mx)\n","    return loss\n","\n","def find_loss_g(y_torch, gen_y_all_torch, z_torch, sigma_w, sigma_u, M):\n","    n = z_torch.shape[0]\n","    d_y = y_torch.shape[1]\n","    w_mx = torch.linalg.vector_norm(z_torch.repeat(n, 1, 1) - torch.swapaxes(z_torch.repeat(n, 1, 1), 0, 1), ord=2, dim=2) ** 2\n","    w_mx = torch.exp(-w_mx / sigma_w)\n","\n","    u_mx_1 = torch.exp(-torch.linalg.vector_norm(y_torch.repeat(n, 1, 1) - torch.swapaxes(y_torch.repeat(n, 1, 1), 0, 1), ord=2, dim=2) ** 2 / sigma_u)\n","    u_mx_2 = torch.mean(\n","        torch.exp(-torch.linalg.vector_norm(gen_y_all_torch.repeat(n, 1, 1).reshape(n, n, -1, d_y) - y_torch.repeat(1, n).reshape(n, n, 1, d_y), ord=2, dim=3) ** 2 / sigma_u), dim=2)\n","    u_mx_3 = u_mx_2.T\n","\n","    gen_y_all_torch_rep = gen_y_all_torch.repeat(n, 1, 1).reshape(n, n, -1, d_y)\n","\n","    u_mx_4 = torch.mean(torch.exp(-torch.linalg.vector_norm(gen_y_all_torch_rep - torch.swapaxes(gen_y_all_torch_rep, 0, 1), ord=2, dim=3) ** 2 / sigma_u), dim=2)\n","\n","    u_mx = u_mx_1 - u_mx_2 - u_mx_3 + u_mx_4\n","\n","    FF_mx = u_mx * w_mx * (1 - torch.eye(n).to(device))\n","\n","    loss = 1 / (n) * torch.sum(FF_mx)\n","    return loss\n","\n","\n","def get_Gzx(X, Y, Z, noise_dimension = 50, noise_type = \"normal\", input_var = 1.0/3.0,\n","      sigma_z_l = 1, sigma_x_l = 1,\n","      sigma_z_g = 1, sigma_x_g = 1):\n","\n","    param = {\n","      \"hidden_layer_size\": 128,\n","      \"ReLU_coef\": 0.8,\n","      \"drop_out_p\": 0.05,\n","      \"lambda_2\": 0.25,\n","      \"lambda_3\": 0.001,\n","      \"wgt_decay\": 1e-05,\n","      \"G_lr\": 0.004950377967576449\n","    }\n","\n","    hidden_layer_size = param['hidden_layer_size']\n","    ReLU_coef = param['ReLU_coef']\n","    drop_out_p = param['drop_out_p']\n","    lambda_2 = param['lambda_2']\n","    lambda_3 = param['lambda_3']\n","    wgt_decay = param['wgt_decay']\n","    G_lr = param['G_lr']\n","\n","    # noise_dimension = 50\n","    BN_type = False\n","    lambda_1 = 1\n","    M_train = 10\n","    batch_size = 128\n","    epochs_num = 1000\n","    # noise_type = \"normal\"\n","\n","    input_dimension = Z.shape[1]\n","    output_dimension_y = Y.shape[1]\n","    output_dimension_x = X.shape[1]\n","\n","    G_zx = Generator(input_dimension, output_dimension_x, noise_dimension, hidden_layer_size, BN_type, ReLU_coef, drop_out_p).to(device)\n","    G_zx_solver = optim.Adam(G_zx.parameters(), lr=G_lr, betas=(0.5, 0.999), weight_decay=wgt_decay)\n","\n","    train_xyz = DatasetSelect_GAN(X, Y, Z, batch_size)\n","    DataLoader = torch.utils.data.DataLoader(train_xyz, batch_size=batch_size, shuffle=True)\n","\n","\n","    G_zx = G_zx.train()\n","\n","    for epoch in range(epochs_num):\n","        # print('EPOCH: ', (epoch+1))\n","        batch_count = 0\n","        G_zx = G_zx.train()\n","        for X_real, Y_real, Z_real, Z_fake in DataLoader:\n","            X_real = X_real.to(device)\n","            Y_real = Y_real.to(device)\n","            Z_real = Z_real.to(device)\n","            Z_fake = Z_fake.to(device)\n","\n","            batch_size = Z_real.shape[0]\n","            Z_real_repeat = Z_real.repeat(M_train,1)\n","\n","            # Generate fake data\n","            Noise_fake = sample_noise(Z_real_repeat.shape[0], noise_dimension, noise_type, input_var = input_var).to(device)\n","            X_fake = G_zx(torch.cat((Z_real_repeat,Noise_fake),dim=1)).to(device)\n","\n","            # X_fake = X_fake.reshape(batch_size, M_train, output_dimension_x)\n","            X_fake = X_fake.reshape(M_train, batch_size, output_dimension_x).swapaxes(0, 1)\n","\n","            # Generator step\n","            g_zx_error = None\n","            G_zx_solver.zero_grad()\n","\n","            l1_regularization = 0\n","\n","            for param in G_zx.parameters():\n","                l1_regularization += torch.linalg.vector_norm(param, ord = 1)\n","\n","            g_zx_error = lambda_1 * find_loss_g(X_real, X_fake, Z_real, sigma_z_g, sigma_x_g, M_train) + \\\n","                    lambda_2 * find_loss_l(X_real, X_fake, Z_real, sigma_z_l, sigma_x_l, M_train) + \\\n","                    lambda_3 * l1_regularization\n","\n","            g_zx_error.backward()\n","            torch.nn.utils.clip_grad_norm_(G_zx.parameters(), max_norm=0.5)\n","            G_zx_solver.step()\n","    return G_zx\n","\n","def get_Gzy(Y, Z):\n","\n","    param = {\n","      \"epochs_num_zy\": 1000,\n","      \"hidden_layer_size_zy\": 256,\n","      \"BN_type_zy\": False,\n","      \"ReLU_coef_zy\": 0.6957593034943316,\n","      \"drop_out_p_zy\": 0.15,\n","      \"G_lr_zy\": 0.00021957327899192503,\n","      \"weight_decay_zy\": 0.00010530996833286099,\n","      \"lambda_1_zy\" : 1,\n","      \"lambda_3_zy\" : 0.020062972725244245\n","    }\n","    MSE_loss = torch.nn.MSELoss()\n","\n","    epochs_num_zy = param['epochs_num_zy']\n","    hidden_layer_size_zy = param['hidden_layer_size_zy']\n","    BN_type_zy = param['BN_type_zy']\n","    ReLU_coef_zy = param['ReLU_coef_zy']\n","    drop_out_p_zy = param['drop_out_p_zy']\n","    G_lr_zy = param['G_lr_zy']\n","    weight_decay_zy = param['weight_decay_zy']\n","    lambda_1_zy = param['lambda_1_zy']\n","    lambda_3_zy = param['lambda_3_zy']\n","\n","    G_zy = Generator(25, 1, 0, hidden_layer_size = hidden_layer_size_zy, BN_type = BN_type_zy, ReLU_coef = ReLU_coef_zy, drop_out_p = drop_out_p_zy).to(device)\n","    G_zy_solver = optim.Adam(G_zy.parameters(), lr=G_lr_zy, betas=(0.5, 0.999), weight_decay=weight_decay_zy)\n","\n","    for epoch in range(epochs_num_zy):\n","        # print('EPOCH: ', (epoch+1))\n","        batch_count = 0\n","        G_zy = G_zy.train()\n","\n","        batch_size = Z.shape[0]\n","\n","        # Generate fake data\n","        Y_fake = G_zy(Z).to(device)\n","\n","        # Generator step\n","        g_zy_error = None\n","        G_zy_solver.zero_grad()\n","        l1_regularization = 0\n","\n","        for param in G_zy.parameters():\n","            l1_regularization += torch.linalg.vector_norm(param, ord = 1)\n","\n","        g_zy_error = lambda_1_zy * MSE_loss(Y_fake, Y) + lambda_3_zy * l1_regularization\n","\n","        g_zy_error.backward()\n","        torch.nn.utils.clip_grad_norm_(G_zy.parameters(), max_norm=0.5)\n","        G_zy_solver.step()\n","\n","    return G_zy\n","\n","def mGAN(n=500, z_dim=2, simulation='type1error', x_dims=2, y_dims=2, a_x=0.05, M=500, k=2, boot_num=1000,\n","     boor_rv_type = \"gaussian\", noise_dimension = 50, noise_type = \"normal\", input_var = 1.0/3.0):\n","\n","    sim_x, sim_y, sim_z = get_xzy_randn_nl(n_points = n, ground_truth = simulation, a = a_x)\n","\n","    x, y, z = sim_x.to(device), sim_y.to(device), sim_z.to(device)\n","\n","    w_mx = torch.linalg.vector_norm(z.repeat(n,1,1) - torch.swapaxes(z.repeat(n,1,1), 0, 1), ord = 1, dim = 2)\n","    sigma_z_l = torch.median(w_mx).item()\n","\n","    v_mx = torch.linalg.vector_norm(x.repeat(n,1,1) - torch.swapaxes(x.repeat(n,1,1), 0, 1), ord = 1, dim = 2)\n","    sigma_x_l = torch.median(v_mx).item()\n","\n","    w_mx = torch.linalg.vector_norm(z.repeat(n,1,1) - torch.swapaxes(z.repeat(n,1,1), 0, 1), ord = 2, dim = 2) ** 2\n","    sigma_z_g = torch.median(w_mx).item() * 2\n","\n","    v_mx = torch.linalg.vector_norm(x.repeat(n,1,1) - torch.swapaxes(x.repeat(n,1,1), 0, 1), ord = 2, dim = 2) ** 2\n","    sigma_x_g = torch.median(v_mx).item() * 2\n","\n","    test_size = int(n/k)\n","    stat_all = torch.zeros(k, 1)\n","    boot_temp_all = torch.zeros(k, boot_num)\n","    cur_k = 0\n","\n","    for k_fold in range(k):\n","        k_fold_start = int(n/k * k_fold)\n","        k_fold_end = int(n/k * (k_fold+1))\n","        X_test, Y_test, Z_test = x[k_fold_start:k_fold_end], y[k_fold_start:k_fold_end], z[k_fold_start:k_fold_end]\n","        X_train, Y_train, Z_train = torch.cat((x[0:k_fold_start], x[k_fold_end:])), torch.cat((y[0:k_fold_start], y[k_fold_end:])), torch.cat((z[0:k_fold_start], z[k_fold_end:]))\n","\n","        G_zx = get_Gzx(X = X_train, Y = Y_train, Z = Z_train, noise_dimension = noise_dimension,\n","                noise_type = noise_type, input_var = input_var,\n","                sigma_z_l = sigma_z_l, sigma_x_l = sigma_x_l,\n","                sigma_z_g = sigma_z_g, sigma_x_g = sigma_x_g)\n","\n","        G_zy = get_Gzy(Y = Y_train, Z = Z_train)\n","\n","        gen_x_all = torch.zeros(test_size, M, x_dims)\n","        gen_y_all = torch.zeros(test_size, y_dims)\n","        z_all = torch.zeros(test_size, z_dim)\n","        x_all = torch.zeros(test_size, x_dims)\n","        y_all = torch.zeros(test_size, y_dims)\n","\n","        G_zx = G_zx.eval()\n","\n","        Z_test_repeat = Z_test.repeat(M,1).to(device)\n","\n","\n","        # Generate fake data\n","        Noise_fake = sample_noise(Z_test_repeat.shape[0], noise_dimension, noise_type, input_var = input_var).to(device)\n","        with torch.no_grad():\n","            gen_x_all = G_zx(torch.cat((Z_test_repeat,Noise_fake),dim=1)).to(device)\n","\n","        # gen_x_all = gen_x_all.reshape(test_size, M, x_dims).detach().to(device)\n","        gen_x_all = gen_x_all.reshape(M, test_size, x_dims).swapaxes(0, 1).detach().to(device)\n","\n","        # Generate fake data\n","        # for i in range(test_size):\n","        #     _, gen_y_all[i,:] = get_xzy_randn_nl_fix(Z = Z_test[i,:], ground_truth = simulation, n_points = M, a = a_x)\n","\n","        # gen_y_all = gen_y_all.reshape(-1,y_dims).to(device)\n","\n","        G_zy = G_zy.eval()\n","        with torch.no_grad():\n","            gen_y_all = G_zy(Z_test.to(device)).to(device)\n","        gen_y_all = gen_y_all.reshape(-1,y_dims).to(device)\n","\n","        gen_x_all = gen_x_all.detach().to(device)\n","        gen_y_all = gen_y_all.detach().to(device)\n","        z_all = Z_test.to(device)\n","        x_all = X_test.to(device)\n","        y_all = Y_test.to(device)\n","\n","        standardise = True\n","\n","        if standardise:\n","            gen_x_all = (gen_x_all - torch.mean(gen_x_all, dim=0, keepdim=True)) / torch.std(gen_x_all, dim=0, keepdim=True)\n","            gen_y_all = (gen_y_all - torch.mean(gen_y_all, dim=0, keepdim=True)) / torch.std(gen_y_all, dim=0, keepdim=True)\n","            x_all = (x_all - torch.mean(x_all, dim=0, keepdim=True)) / torch.std(x_all, dim=0, keepdim=True)\n","            y_all = (y_all - torch.mean(y_all, dim=0, keepdim=True)) / torch.std(y_all, dim=0, keepdim=True)\n","            z_all = (z_all - torch.mean(z_all, dim=0, keepdim=True)) / torch.std(z_all, dim=0, keepdim=True)\n","\n","        cur_stat, cur_boot_temp = get_p_value_stat(boot_num, M, test_size, gen_x_all.to(device), gen_y_all.to(device),\n","                                x_all.to(device), y_all.to(device), z_all.to(device), boor_rv_type)\n","        stat_all[cur_k,:] = cur_stat\n","        boot_temp_all[cur_k,:] = torch.from_numpy(cur_boot_temp)\n","        cur_k = cur_k + 1\n","\n","    return np.mean(torch.mean(boot_temp_all, dim = 0).numpy() > torch.mean(stat_all).item() )\n","\n","def run_experiment(params):\n","    test = params[\"test\"]\n","    sample_size = params[\"sample_size\"]\n","    z_dim = params[\"z_dim\"]\n","    dx = params[\"dx\"]\n","    dy = params[\"dy\"]\n","    n_test = params[\"n_test\"]\n","    alpha_x = params[\"alpha_x\"]\n","    m_value = params[\"m_value\"]\n","    k_value = params[\"k_value\"]\n","    j_value = params[\"j_value\"]\n","    alpha = params[\"alpha\"]\n","    alpha1 = params[\"alpha1\"]\n","    set_seeds = params[\"set_seeds\"]\n","    boor_rv_type = params[\"boor_rv_type\"]\n","\n","\n","    np.random.seed(set_seeds)\n","    torch.manual_seed(set_seeds)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(set_seeds)\n","\n","    p_values = np.array([])\n","    test_count = 0\n","    if test == 'H0':\n","        for n in tqdm(range(n_test)):\n","            # start_time = datetime.now()\n","\n","            p_value = mGAN(n=sample_size, z_dim=z_dim, simulation=test, x_dims=dx, y_dims=dy, a_x=alpha_x, M=m_value,\n","                    k=k_value, boot_num=j_value, boor_rv_type = boor_rv_type)\n","\n","            # test_count += 1\n","            # print(\"--- The %d'th iteration take %s seconds ---\" % (test_count, (datetime.now() - start_time)))\n","\n","            p_values = np.append(p_values, p_value)\n","            fp = [pval < alpha  for pval in p_values]\n","            final_result = np.mean(fp)\n","            fp1 = [pval < alpha1 for pval in p_values]\n","            final_result1 = np.mean(fp1)\n","\n","            # print('The stat is {}'.format(p_value))\n","            # print('Type 1 error: {} for z dimension {} with significance level {}'.format(final_result, z_dim, alpha))\n","            # print('Type 1 error: {} for z dimension {} with significance level {}'.format(final_result1, z_dim, alpha1))\n","\n","            final_result_list = np.array([final_result])\n","            final_result1_list = np.array([final_result1])\n","\n","        print('Type 1 error: {} with significance level {}'.format(final_result, alpha))\n","        print('Type 1 error: {} with significance level {}'.format(final_result1, alpha1))\n","\n","    if test == 'H1_dense' or test == 'H1_sparse':\n","        for n in tqdm(range(n_test)):\n","            # start_time = datetime.now()\n","\n","            p_value = mGAN(n=sample_size, z_dim=z_dim, simulation=test, x_dims=dx, y_dims=dy, a_x=alpha_x, M=m_value,\n","                    k=k_value, boot_num=j_value, boor_rv_type = boor_rv_type)\n","\n","            # test_count += 1\n","            # print(\"--- The %d'th iteration take %s seconds ---\" % (test_count, (datetime.now() - start_time)))\n","\n","            p_values = np.append(p_values, p_value)\n","            fp = [pval < alpha  for pval in p_values]\n","            final_result = np.mean(fp)\n","            fp1 = [pval < alpha1 for pval in p_values]\n","            final_result1 = np.mean(fp1)\n","\n","            # print('The stat is {}'.format(p_value))\n","            # print('Power: {} for z dimension {} with significance level {}'.format(final_result, z_dim, alpha))\n","            # print('Power: {} for z dimension {} with significance level {}'.format(final_result1, z_dim, alpha1))\n","\n","            final_result_list = np.array([final_result])\n","            final_result1_list = np.array([final_result1])\n","\n","        print('Power: {} with significance level {}'.format(final_result, alpha))\n","        print('Power: {} with significance level {}'.format(final_result1, alpha1))\n","\n","    return p_values"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P8aLjjAvTe61","executionInfo":{"status":"ok","timestamp":1733514123663,"user_tz":360,"elapsed":3351311,"user":{"displayName":"Linjun","userId":"13995434670626730968"}},"outputId":"4936ef37-d1da-4319-bbff-3ea3e7716bb3"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["---                     ---\n","---                     ---\n","--- The n =  400  case ---\n","--- The H0 case ---\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 500/500 [5:45:08<00:00, 41.42s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Type 1 error: 0.13 with significance level 0.1\n","Type 1 error: 0.07 with significance level 0.05\n","--- The H1_dense case Empirical Power ---\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 500/500 [5:44:02<00:00, 41.29s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Power: 0.784 with significance level 0.1\n","Power: 0.648 with significance level 0.05\n","Adjusted Power: 0.744 with significance level 0.084\n","Adjusted Power: 0.578 with significance level 0.03495000000000001\n","--- The H1_sparse case Empirical Power ---\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 500/500 [5:42:20<00:00, 41.08s/it]"]},{"output_type":"stream","name":"stdout","text":["Power: 0.932 with significance level 0.1\n","Power: 0.812 with significance level 0.05\n","Adjusted Power: 0.906 with significance level 0.084\n","Adjusted Power: 0.762 with significance level 0.03495000000000001\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["print(\"---                     ---\")\n","print(\"---                     ---\")\n","print(\"--- The n = \",input_sample_size,\" case ---\")\n","param = {\n","  \"test\": \"H0\", # ['H0', 'H1_dense', 'H1_sparse']\n","  \"sample_size\":  input_sample_size, # [100, 200, 300, 400]\n","  \"z_dim\": 25, # [5, 50, 250]\n","  \"dx\": 25,\n","  \"dy\": 1,\n","  \"n_test\": input_n_test, # [500] in the paper\n","  \"alpha_x\": 0.50, # only used under alternative ['power_sparse': a = 0.5; 'power_dense': 1/sqrt(2*p2) {1/math.sqrt(2*25)}]\n","  \"m_value\": 100, # [500]\n","  \"k_value\": 2, # [1, 2, 4]\n","  \"j_value\": 1000, # [1000, 2000]\n","  \"alpha\": 0.1,\n","  \"alpha1\": 0.05,\n","  \"set_seeds\": input_seed,\n","  \"boor_rv_type\":  'rademacher' # ['rademacher', 'gaussian']\n","}\n","\n","print(\"--- The H0 case ---\")\n","p_val_list = run_experiment(param)\n","\n","import numpy as np\n","quantile_5, quantile_10 = np.quantile(p_val_list, 0.05), np.quantile(p_val_list, 0.10)\n","\n","param = {\n","  \"test\": \"H1_dense\", # ['H0', 'H1_dense', 'H1_sparse']\n","  \"sample_size\":  input_sample_size, # [100, 200, 300, 400]\n","  \"z_dim\": 25, # [5, 50, 250]\n","  \"dx\": 25,\n","  \"dy\": 1,\n","  \"n_test\": input_n_test, # [500] in the paper\n","  \"alpha_x\": 0.50, # only used under alternative ['power_sparse': a = 0.5; 'power_dense': 1/sqrt(2*p2) {1/math.sqrt(2*25)}]\n","  \"m_value\": 100, # [500]\n","  \"k_value\": 2, # [1, 2, 4]\n","  \"j_value\": 1000, # [1000, 2000]\n","  \"alpha\": 0.1,\n","  \"alpha1\": 0.05,\n","  \"set_seeds\": input_seed,\n","  \"boor_rv_type\":  'rademacher' # ['rademacher', 'gaussian']\n","}\n","\n","print(\"--- The H1_dense case Empirical Power ---\")\n","p_val_list_H1_dense = run_experiment(param)\n","\n","fp = [pval < quantile_10  for pval in p_val_list_H1_dense]\n","final_result = np.mean(fp)\n","fp1 = [pval < quantile_5 for pval in p_val_list_H1_dense]\n","final_result1 = np.mean(fp1)\n","\n","\n","print('Adjusted Power: {} with significance level {}'.format(final_result, quantile_10))\n","print('Adjusted Power: {} with significance level {}'.format(final_result1, quantile_5))\n","\n","param = {\n","  \"test\": \"H1_sparse\", # ['H0', 'H1_dense', 'H1_sparse']\n","  \"sample_size\":  input_sample_size, # [100, 200, 300, 400]\n","  \"z_dim\": 25, # [5, 50, 250]\n","  \"dx\": 25,\n","  \"dy\": 1,\n","  \"n_test\": input_n_test, # [500] in the paper\n","  \"alpha_x\": 0.50, # only used under alternative ['power_sparse': a = 0.5; 'power_dense': 1/sqrt(2*p2) {1/math.sqrt(2*25)}]\n","  \"m_value\": 100, # [500]\n","  \"k_value\": 2, # [1, 2, 4]\n","  \"j_value\": 1000, # [1000, 2000]\n","  \"alpha\": 0.1,\n","  \"alpha1\": 0.05,\n","  \"set_seeds\": input_seed,\n","  \"boor_rv_type\":  'rademacher' # ['rademacher', 'gaussian']\n","}\n","\n","print(\"--- The H1_sparse case Empirical Power ---\")\n","p_val_list_H1_sparse = run_experiment(param)\n","\n","fp = [pval < quantile_10  for pval in p_val_list_H1_sparse]\n","final_result = np.mean(fp)\n","fp1 = [pval < quantile_5 for pval in p_val_list_H1_sparse]\n","final_result1 = np.mean(fp1)\n","\n","\n","print('Adjusted Power: {} with significance level {}'.format(final_result, quantile_10))\n","print('Adjusted Power: {} with significance level {}'.format(final_result1, quantile_5))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}